{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-16T05:27:38.872329Z",
     "iopub.status.busy": "2025-07-16T05:27:38.872068Z",
     "iopub.status.idle": "2025-07-16T05:29:50.846263Z",
     "shell.execute_reply": "2025-07-16T05:29:50.845486Z",
     "shell.execute_reply.started": "2025-07-16T05:27:38.872302Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Cloning into 'MotionDirector'...\n",
      "remote: Enumerating objects: 657, done.\u001b[K\n",
      "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
      "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
      "remote: Total 657 (delta 108), reused 88 (delta 81), pack-reused 494 (from 1)\u001b[K\n",
      "Receiving objects: 100% (657/657), 132.29 MiB | 50.34 MiB/s, done.\n",
      "Resolving deltas: 100% (349/349), done.\n",
      "/content/MotionDirector\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for lora_diffusion (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for loralib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 4.25.8 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "%cd /content\n",
    "!git clone https://github.com/danhtran2mind/MotionDirector\n",
    "%cd MotionDirector\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T05:29:50.848033Z",
     "iopub.status.busy": "2025-07-16T05:29:50.847771Z",
     "iopub.status.idle": "2025-07-16T05:29:54.955247Z",
     "shell.execute_reply": "2025-07-16T05:29:54.954373Z",
     "shell.execute_reply.started": "2025-07-16T05:29:50.848010Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T05:29:54.956819Z",
     "iopub.status.busy": "2025-07-16T05:29:54.956511Z",
     "iopub.status.idle": "2025-07-16T05:29:54.963707Z",
     "shell.execute_reply": "2025-07-16T05:29:54.962891Z",
     "shell.execute_reply.started": "2025-07-16T05:29:54.956786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def copy_file_pairs(source_dir, dest_dir, max_pairs=20, seed=None):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    mp4_files = [f for f in os.listdir(source_dir) if f.endswith('.mp4')]\n",
    "    selected_mp4_files = random.sample(mp4_files, min(len(mp4_files), max_pairs))\n",
    "    for mp4 in selected_mp4_files:\n",
    "        base = os.path.splitext(mp4)[0]\n",
    "        txt = f\"{base}.txt\"\n",
    "        if os.path.exists(os.path.join(source_dir, txt)):\n",
    "            shutil.copy2(os.path.join(source_dir, mp4), os.path.join(dest_dir, mp4))\n",
    "            shutil.copy2(os.path.join(source_dir, txt), os.path.join(dest_dir, txt))\n",
    "    return len(selected_mp4_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T05:29:54.965605Z",
     "iopub.status.busy": "2025-07-16T05:29:54.965374Z",
     "iopub.status.idle": "2025-07-16T05:30:00.766653Z",
     "shell.execute_reply": "2025-07-16T05:30:00.766019Z",
     "shell.execute_reply.started": "2025-07-16T05:29:54.965578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0f51df7a5047bd8d404fca30add463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b800f95e15fb4064854902602aa3a3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6cc4ba523f4a18adbc9ffaa3525340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "studio_ghibli_wan14b_t2v_v01_dataset.zip:   0%|          | 0.00/300M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"data/ghibli/raw\", exist_ok=True)\n",
    "\n",
    "# Download the dataset using snapshot_download\n",
    "snapshot_download(repo_id=\"raymondt/ghibi_t2v\", \n",
    "                 local_dir=\"data/ghibli/raw\", \n",
    "                 repo_type=\"dataset\")\n",
    "\n",
    "# Assuming the zip file is downloaded, unzip it to the target directory\n",
    "import zipfile\n",
    "zip_path = \"data/ghibli/raw/studio_ghibli_wan14b_t2v_v01_dataset.zip\"\n",
    "extract_path = \"data/ghibli/raw\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T05:30:00.767663Z",
     "iopub.status.busy": "2025-07-16T05:30:00.767419Z",
     "iopub.status.idle": "2025-07-16T05:30:01.056828Z",
     "shell.execute_reply": "2025-07-16T05:30:01.056142Z",
     "shell.execute_reply.started": "2025-07-16T05:30:00.767643Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 240 pairs to data/ghibli/videos\n"
     ]
    }
   ],
   "source": [
    "# Copy the videos directory to the desired location\n",
    "source = \"data/ghibli/raw/videos/1920x1040\"\n",
    "dest = \"data/ghibli/videos\"\n",
    "\n",
    "copied = copy_file_pairs(source, dest, max_pairs=240, seed=42)\n",
    "print(f\"Copied {copied} pairs to {dest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T05:30:01.057811Z",
     "iopub.status.busy": "2025-07-16T05:30:01.057583Z",
     "iopub.status.idle": "2025-07-16T05:30:29.547286Z",
     "shell.execute_reply": "2025-07-16T05:30:29.546428Z",
     "shell.execute_reply.started": "2025-07-16T05:30:01.057784Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bbd6ea501745bcabdb7f89bdb7af95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b20bb02614d4dfdb037484f1bbf06ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d127ca77964c7a9ab58b55dced9701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9272edd1784c12baaa2826a1effcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7a542514544640af6d140501a7a05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33b170a93dd430386b8bd5f045f3419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64f3a8914004c2283450052563fa0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7909289ad6a747f4a6fbd81db836e7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d562fad6588646f3929d7dfd805e001e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/681M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4361967a684e7c99075ba14b7ec864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a17f9d827c4656b1ae272d679b0a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/737 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ebea88cba841d0a117156384a7af6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bc8fb35a9f44deb336bb1b109298ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_pytorch_model.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba9dca7dfb2455aaf4627d43390d550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.bin:   0%|          | 0.00/2.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab2fd449c9b44a58ff281b30deb6e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aca918dcb4f4b4b9f6804859ea65ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text2video_pytorch_model.pth:   0%|          | 0.00/2.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf61eb3a364e455b89379e66e8f304d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.bin:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/content/MotionDirector/ckpts/zeroscope_v2_576w'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "# Download ZeroScope model snapshot\n",
    "repo_id = \"cerspense/zeroscope_v2_576w\"\n",
    "snapshot_download(repo_id=repo_id,\n",
    "                  local_dir=\"./ckpts/zeroscope_v2_576w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-15T14:12:26.903891Z",
     "iopub.status.busy": "2025-07-15T14:12:26.903623Z",
     "iopub.status.idle": "2025-07-15T14:12:26.907531Z",
     "shell.execute_reply": "2025-07-15T14:12:26.906880Z",
     "shell.execute_reply.started": "2025-07-15T14:12:26.903873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T05:35:53.783879Z",
     "iopub.status.busy": "2025-07-16T05:35:53.783504Z",
     "iopub.status.idle": "2025-07-16T05:35:53.791697Z",
     "shell.execute_reply": "2025-07-16T05:35:53.791129Z",
     "shell.execute_reply.started": "2025-07-16T05:35:53.783849Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs/config_multi_videos.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/config_multi_videos.yaml\n",
    "# Pretrained diffusers model path.\n",
    "pretrained_model_path: \"./ckpts/zeroscope_v2_576w\"\n",
    "# pretrained_model_path: \"./ckpts/text-to-video-ms-1.7b\"\n",
    "# The folder where your training outputs will be placed.\n",
    "output_dir: \"./zeroscope_v2_576w-Ghibli-LoRA\"\n",
    "# resume_step: 500\n",
    "# resume_from_checkpoint: \"./zeroscope_v2_576w-Scenery_Anime_Bright-lora/train_2025-07-10T13-46-57\"\n",
    "# lora_path: \"zeroscope_v2_576w-Scenery_Anime_Bright-lora/checkpoint-500\" # This argument is used for training resumption\n",
    "# lora_path: zeroscope_v2_576w-Ghibli-LoRA/train_2025-07-13T06-46-47/checkpoint-200\n",
    "\n",
    "dataset_types:\n",
    "  - 'folder'\n",
    "\n",
    "# Caches the latents (Frames-Image -> VAE -> Latent) to a HDD or SDD.\n",
    "# The latents will be saved under your training folder, and loaded automatically for training.\n",
    "# This both saves memory and speeds up training and takes very little disk space.\n",
    "cache_latents: True\n",
    "\n",
    "\n",
    "# If you have cached latents set to `True` and have a directory of cached latents,\n",
    "# you can skip the caching process and load previously saved ones.\n",
    "cached_latent_dir: null #/path/to/cached_latents\n",
    "# cached_latent_dir: zeroscope_v2_576w-Ghibli-LoRA/train_2025-07-13T06-46-47/cached_latents\n",
    "\n",
    "# Use LoRA for the UNET model.\n",
    "use_unet_lora: True\n",
    "\n",
    "# LoRA Dropout. This parameter adds the probability of randomly zeros out elements. Helps prevent overfitting.\n",
    "# See: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "lora_unet_dropout: 0.1\n",
    "\n",
    "# Choose whether or not ito save the full pretrained model weights for both checkpoints and after training.\n",
    "# The only time you want this off is if you're doing full LoRA training.\n",
    "save_pretrained_model: True\n",
    "# save_pretrained_model: True\n",
    "\n",
    "# The rank for LoRA training. With ModelScope, the maximum should be 1024.\n",
    "# VRAM increases with higher rank, lower when decreased.\n",
    "lora_rank: 16\n",
    "\n",
    "# Training data parameters\n",
    "train_data:\n",
    "  # 'multiple videos'\n",
    "  path: \"./data/ghibli/videos\"\n",
    "  # The width and height in which you want your training data to be resized to.\n",
    "  width: 384\n",
    "  height: 384\n",
    "\n",
    "  # This will find the closest aspect ratio to your input width and height.\n",
    "  # For example, 512x512 width and height with a video of resolution 1280x720 will be resized to 512x256\n",
    "  use_bucketing: True\n",
    "  gradient_accumulation_steps: 2\n",
    "  batch_size: 1\n",
    "  # The start frame index where your videos should start (Leave this at one for json and folder based training).\n",
    "  sample_start_idx: 1\n",
    "\n",
    "  # Used for 'folder'. The rate at which your frames are sampled. Does nothing for 'json' and 'single_video' dataset.\n",
    "  fps: 16\n",
    "\n",
    "  # For 'single_video' and 'json'. The number of frames to \"step\" (1,2,3,4) (frame_step=2) -> (1,3,5,7, ...).\n",
    "  frame_step: 1\n",
    "\n",
    "  # The number of frames to sample. The higher this number, the higher the VRAM (acts similar to batch size).\n",
    "  n_sample_frames: 24\n",
    "\n",
    "  # The prompt when using a a single video file\n",
    "  # fallback_prompt: \"A person is riding a bicycle.\"\n",
    "\n",
    "# Validation data parameters.\n",
    "validation_data:\n",
    "  # A custom prompt that is different from your training dataset.\n",
    "  prompt:\n",
    "  - \"Studio Ghibli style. The video showcases a vibrant and lively scene set in the early.\"\n",
    "  - \"Studio Ghibli style. A woman with black hair is holding a gun in her hand.\"\n",
    "\n",
    "  # Whether or not to sample preview during training (Requires more VRAM).\n",
    "  # sample_preview: True\n",
    "  sample_preview: False\n",
    "\n",
    "  # The number of frames to sample during validation.\n",
    "  num_frames: 24\n",
    "\n",
    "  # Height and width of validation sample.\n",
    "  width: 384\n",
    "  height: 384\n",
    "\n",
    "  # Number of inference steps when generating the video.\n",
    "  num_inference_steps: 15\n",
    "\n",
    "  # CFG scale\n",
    "  guidance_scale: 12\n",
    "\n",
    "  # scale of spatial LoRAs, default is 0\n",
    "  spatial_scale: 0\n",
    "\n",
    "  # scale of noise prior, i.e. the scale of inversion noises\n",
    "  noise_prior: 0\n",
    "\n",
    "use_offset_noise: False\n",
    "offset_noise_strength: 0.\n",
    "\n",
    "# Learning rate for AdamW\n",
    "learning_rate: 5e-4\n",
    "\n",
    "# Weight decay. Higher = more regularization. Lower = closer to dataset.\n",
    "adam_weight_decay: 1e-4\n",
    "\n",
    "# Maximum number of train steps. Model is saved after training.\n",
    "max_train_steps: 5000\n",
    "\n",
    "# Saves a model every nth step.\n",
    "checkpointing_steps: 5000\n",
    "\n",
    "# How many steps to do for validation if sample_preview is enabled.\n",
    "validation_steps: 5000\n",
    "\n",
    "# Whether or not we want to use mixed precision with accelerate\n",
    "mixed_precision: \"fp16\"\n",
    "# mixed_precision: \"no\"\n",
    "\n",
    "# Trades VRAM usage for speed. You lose roughly 20% of training speed, but save a lot of VRAM.\n",
    "# If you need to save more VRAM, it can also be enabled for the text encoder, but reduces speed x2.\n",
    "gradient_checkpointing: True\n",
    "text_encoder_gradient_checkpointing: True\n",
    "\n",
    "# Xformers must be installed for best memory savings and performance (< Pytorch 2.0)\n",
    "enable_xformers_memory_efficient_attention: True\n",
    "use_8bit_adam: True\n",
    "\n",
    "# Use scaled dot product attention (Only available with >= Torch 2.0)\n",
    "enable_torch_2_attn: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T05:36:02.733520Z",
     "iopub.status.busy": "2025-07-16T05:36:02.732856Z",
     "iopub.status.idle": "2025-07-16T16:01:06.692095Z",
     "shell.execute_reply": "2025-07-16T16:01:06.688451Z",
     "shell.execute_reply.started": "2025-07-16T05:36:02.733496Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-16 05:36:13.391674: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752644173.574411     316 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752644173.625685     316 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Initializing the conversion map\n",
      "{'rescale_betas_zero_snr', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "An error occurred while trying to fetch ./ckpts/zeroscope_v2_576w: Error no file named diffusion_pytorch_model.safetensors found in directory ./ckpts/zeroscope_v2_576w.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "{'latents_mean', 'use_post_quant_conv', 'mid_block_add_attention', 'force_upcast', 'use_quant_conv', 'shift_factor', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at ./ckpts/zeroscope_v2_576w.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "An error occurred while trying to fetch ./ckpts/zeroscope_v2_576w: Error no file named diffusion_pytorch_model.safetensors found in directory ./ckpts/zeroscope_v2_576w.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "All model checkpoint weights were used when initializing UNet3DConditionModel.\n",
      "\n",
      "All the weights of UNet3DConditionModel were initialized from the model checkpoint at ./ckpts/zeroscope_v2_576w.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet3DConditionModel for predictions without further training.\n",
      "Could not enable memory efficient attention for xformers or Torch 2.0.\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of ./ckpts/zeroscope_v2_576w.\n",
      "Loading pipeline components...:  40%|█████▏       | 2/5 [00:00<00:01,  2.52it/s]{'rescale_betas_zero_snr', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of ./ckpts/zeroscope_v2_576w.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of ./ckpts/zeroscope_v2_576w.\n",
      "Loading pipeline components...: 100%|█████████████| 5/5 [00:00<00:00,  5.76it/s]\n",
      "Expected types for unet: (<class 'diffusers.models.unets.unet_3d_condition.UNet3DConditionModel'>,), got <class 'models.unet_3d_condition.UNet3DConditionModel'>.\n",
      "The TextToVideoSDPipeline has been deprecated and will not receive bug fixes or feature updates after Diffusers version 0.33.1. \n",
      "Caching Latents.: 100%|███████████████████████| 240/240 [09:41<00:00,  2.42s/it]\n",
      "Lora successfully injected into UNet3DConditionModel.\n",
      "Lora successfully injected into UNet3DConditionModel.\n",
      "unet._set_gradient_checkpointing(unet_enable)\n",
      "Steps:   0%|                                           | 0/5000 [00:00<?, ?it/s]1942 params have been unfrozen for training.\n",
      "/usr/local/lib/python3.11/dist-packages/diffusers/models/transformers/transformer_2d.py:35: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n",
      "Steps: 100%|█████████████████████████████| 5000/5000 [10:14:13<00:00,  7.28s/it][2025-07-16 16:00:44,146] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-07-16 16:00:46,892] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/5 [00:00<?, ?it/s]\u001b[A{'rescale_betas_zero_snr', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of ./ckpts/zeroscope_v2_576w.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of ./ckpts/zeroscope_v2_576w.\n",
      "Loading pipeline components...: 100%|█████████████| 5/5 [00:00<00:00, 50.50it/s]\n",
      "Expected types for unet: (<class 'diffusers.models.unets.unet_3d_condition.UNet3DConditionModel'>,), got <class 'models.unet_3d_condition.UNet3DConditionModel'>.\n",
      "The TextToVideoSDPipeline has been deprecated and will not receive bug fixes or feature updates after Diffusers version 0.33.1. \n",
      "Configuration saved in ./zeroscope_v2_576w-Ghibli-LoRA/train_2025-07-16T05-36-26/checkpoint-5000/vae/config.json\n",
      "Model weights saved in ./zeroscope_v2_576w-Ghibli-LoRA/train_2025-07-16T05-36-26/checkpoint-5000/vae/diffusion_pytorch_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "!python main_train.py --config ./configs/config_multi_videos.yaml"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
